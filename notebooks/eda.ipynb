{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14530cab5df8d156",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T15:24:40.958612Z",
     "start_time": "2025-11-01T15:24:40.645162Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Visualization libraries\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpress\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpx\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS AND SETUP\n",
    "# ============================================================================\n",
    "# Standard library imports\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.append(\"..\")  # adjust based on notebook location\n",
    "\n",
    "# Data management imports\n",
    "from src.data.data_manager import (\n",
    "    load_raw, save_interim, save_processed,\n",
    "    load_latest_interim, load_latest_processed\n",
    ")\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Statistics and ML\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14d6559bbe84d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T15:24:44.024783Z",
     "start_time": "2025-11-01T15:24:41.426894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading raw file: /Users/unclesam/Projects/supply-chain-ml-project/data/raw/DataCoSupplyChainDataset.csv\n",
      "‚ö†Ô∏è UTF-8 decode failed. Retrying with Latin-1...\n",
      "================================================================================\n",
      "DATASET LOADED SUCCESSFULLY\n",
      "================================================================================\n",
      "\n",
      "üìä Dataset Shape: 180,519 rows √ó 53 columns\n",
      "üíæ Memory Usage: 332.64 MB\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOAD RAW DATASET\n",
    "# ============================================================================\n",
    "# Load the raw dataset using the data manager\n",
    "# This will automatically download if the file is missing or outdated\n",
    "df = load_raw()\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"=\"*80)\n",
    "print(\"DATASET LOADED SUCCESSFULLY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüìä Dataset Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"üíæ Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526d0d8f29786423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Days for shipping (real)</th>\n",
       "      <th>Days for shipment (scheduled)</th>\n",
       "      <th>Benefit per order</th>\n",
       "      <th>Sales per customer</th>\n",
       "      <th>Delivery Status</th>\n",
       "      <th>Late_delivery_risk</th>\n",
       "      <th>Category Id</th>\n",
       "      <th>Category Name</th>\n",
       "      <th>Customer City</th>\n",
       "      <th>...</th>\n",
       "      <th>Order Zipcode</th>\n",
       "      <th>Product Card Id</th>\n",
       "      <th>Product Category Id</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>Product Image</th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Product Price</th>\n",
       "      <th>Product Status</th>\n",
       "      <th>shipping date (DateOrders)</th>\n",
       "      <th>Shipping Mode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DEBIT</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>91.250000</td>\n",
       "      <td>314.640015</td>\n",
       "      <td>Advance shipping</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>Sporting Goods</td>\n",
       "      <td>Caguas</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1360</td>\n",
       "      <td>73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://images.acmesports.sports/Smart+watch</td>\n",
       "      <td>Smart watch</td>\n",
       "      <td>327.75</td>\n",
       "      <td>0</td>\n",
       "      <td>2/3/2018 22:56</td>\n",
       "      <td>Standard Class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRANSFER</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>-249.089996</td>\n",
       "      <td>311.359985</td>\n",
       "      <td>Late delivery</td>\n",
       "      <td>1</td>\n",
       "      <td>73</td>\n",
       "      <td>Sporting Goods</td>\n",
       "      <td>Caguas</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1360</td>\n",
       "      <td>73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://images.acmesports.sports/Smart+watch</td>\n",
       "      <td>Smart watch</td>\n",
       "      <td>327.75</td>\n",
       "      <td>0</td>\n",
       "      <td>1/18/2018 12:27</td>\n",
       "      <td>Standard Class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CASH</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>-247.779999</td>\n",
       "      <td>309.720001</td>\n",
       "      <td>Shipping on time</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>Sporting Goods</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1360</td>\n",
       "      <td>73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://images.acmesports.sports/Smart+watch</td>\n",
       "      <td>Smart watch</td>\n",
       "      <td>327.75</td>\n",
       "      <td>0</td>\n",
       "      <td>1/17/2018 12:06</td>\n",
       "      <td>Standard Class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DEBIT</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>22.860001</td>\n",
       "      <td>304.809998</td>\n",
       "      <td>Advance shipping</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>Sporting Goods</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1360</td>\n",
       "      <td>73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://images.acmesports.sports/Smart+watch</td>\n",
       "      <td>Smart watch</td>\n",
       "      <td>327.75</td>\n",
       "      <td>0</td>\n",
       "      <td>1/16/2018 11:45</td>\n",
       "      <td>Standard Class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>134.210007</td>\n",
       "      <td>298.250000</td>\n",
       "      <td>Advance shipping</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>Sporting Goods</td>\n",
       "      <td>Caguas</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1360</td>\n",
       "      <td>73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://images.acmesports.sports/Smart+watch</td>\n",
       "      <td>Smart watch</td>\n",
       "      <td>327.75</td>\n",
       "      <td>0</td>\n",
       "      <td>1/15/2018 11:24</td>\n",
       "      <td>Standard Class</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Type  Days for shipping (real)  Days for shipment (scheduled)  \\\n",
       "0     DEBIT                         3                              4   \n",
       "1  TRANSFER                         5                              4   \n",
       "2      CASH                         4                              4   \n",
       "3     DEBIT                         3                              4   \n",
       "4   PAYMENT                         2                              4   \n",
       "\n",
       "   Benefit per order  Sales per customer   Delivery Status  \\\n",
       "0          91.250000          314.640015  Advance shipping   \n",
       "1        -249.089996          311.359985     Late delivery   \n",
       "2        -247.779999          309.720001  Shipping on time   \n",
       "3          22.860001          304.809998  Advance shipping   \n",
       "4         134.210007          298.250000  Advance shipping   \n",
       "\n",
       "   Late_delivery_risk  Category Id   Category Name Customer City  ...  \\\n",
       "0                   0           73  Sporting Goods        Caguas  ...   \n",
       "1                   1           73  Sporting Goods        Caguas  ...   \n",
       "2                   0           73  Sporting Goods      San Jose  ...   \n",
       "3                   0           73  Sporting Goods   Los Angeles  ...   \n",
       "4                   0           73  Sporting Goods        Caguas  ...   \n",
       "\n",
       "  Order Zipcode Product Card Id Product Category Id  Product Description  \\\n",
       "0           NaN            1360                  73                  NaN   \n",
       "1           NaN            1360                  73                  NaN   \n",
       "2           NaN            1360                  73                  NaN   \n",
       "3           NaN            1360                  73                  NaN   \n",
       "4           NaN            1360                  73                  NaN   \n",
       "\n",
       "                                  Product Image  Product Name Product Price  \\\n",
       "0  http://images.acmesports.sports/Smart+watch   Smart watch         327.75   \n",
       "1  http://images.acmesports.sports/Smart+watch   Smart watch         327.75   \n",
       "2  http://images.acmesports.sports/Smart+watch   Smart watch         327.75   \n",
       "3  http://images.acmesports.sports/Smart+watch   Smart watch         327.75   \n",
       "4  http://images.acmesports.sports/Smart+watch   Smart watch         327.75   \n",
       "\n",
       "  Product Status shipping date (DateOrders)   Shipping Mode  \n",
       "0              0             2/3/2018 22:56  Standard Class  \n",
       "1              0            1/18/2018 12:27  Standard Class  \n",
       "2              0            1/17/2018 12:06  Standard Class  \n",
       "3              0            1/16/2018 11:45  Standard Class  \n",
       "4              0            1/15/2018 11:24  Standard Class  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a8d92284cece65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Type', 'Days for shipping (real)', 'Days for shipment (scheduled)',\n",
       "       'Benefit per order', 'Sales per customer', 'Delivery Status',\n",
       "       'Late_delivery_risk', 'Category Id', 'Category Name', 'Customer City',\n",
       "       'Customer Country', 'Customer Email', 'Customer Fname', 'Customer Id',\n",
       "       'Customer Lname', 'Customer Password', 'Customer Segment',\n",
       "       'Customer State', 'Customer Street', 'Customer Zipcode',\n",
       "       'Department Id', 'Department Name', 'Latitude', 'Longitude', 'Market',\n",
       "       'Order City', 'Order Country', 'Order Customer Id',\n",
       "       'order date (DateOrders)', 'Order Id', 'Order Item Cardprod Id',\n",
       "       'Order Item Discount', 'Order Item Discount Rate', 'Order Item Id',\n",
       "       'Order Item Product Price', 'Order Item Profit Ratio',\n",
       "       'Order Item Quantity', 'Sales', 'Order Item Total',\n",
       "       'Order Profit Per Order', 'Order Region', 'Order State', 'Order Status',\n",
       "       'Order Zipcode', 'Product Card Id', 'Product Category Id',\n",
       "       'Product Description', 'Product Image', 'Product Name', 'Product Price',\n",
       "       'Product Status', 'shipping date (DateOrders)', 'Shipping Mode'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6c01089d4cf899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA OVERVIEW - First Look\n",
    "# ============================================================================\n",
    "# Display the first few rows to get a sense of the data\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(\"=\"*80)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2360a5ca6a39031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COLUMN INFORMATION\n",
    "# ============================================================================\n",
    "# Display all column names with their data types\n",
    "print(\"Dataset Columns and Data Types:\")\n",
    "print(\"=\"*80)\n",
    "column_info = pd.DataFrame({\n",
    "    'Column Name': df.columns,\n",
    "    'Data Type': df.dtypes,\n",
    "    'Non-Null Count': df.count(),\n",
    "    'Null Count': df.isnull().sum(),\n",
    "    'Null Percentage': (df.isnull().sum() / len(df) * 100).round(2)\n",
    "})\n",
    "print(column_info.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5ce0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics for Numeric Columns:\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummary Statistics for Numeric Columns:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m numeric_cols \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mnumber])\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTotal numeric columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(numeric_cols)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m df[numeric_cols]\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# BASIC STATISTICS FOR NUMERIC COLUMNS\n",
    "# ============================================================================\n",
    "# Display summary statistics for numeric columns\n",
    "print(\"Summary Statistics for Numeric Columns:\")\n",
    "print(\"=\"*80)\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"\\nTotal numeric columns: {len(numeric_cols)}\")\n",
    "df[numeric_cols].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e8ccaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CATEGORICAL COLUMNS OVERVIEW\n",
    "# ============================================================================\n",
    "# Identify categorical columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "print(\"Categorical Columns Overview:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal categorical columns: {len(categorical_cols)}\")\n",
    "print(\"\\nUnique value counts for each categorical column:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Display unique value counts for each categorical column\n",
    "for col in categorical_cols:\n",
    "    unique_count = df[col].nunique()\n",
    "    print(f\"{col:40s}: {unique_count:6d} unique values\")\n",
    "    if unique_count <= 20:  # Show value counts if not too many unique values\n",
    "        print(f\"  Top values: {dict(df[col].value_counts().head(5))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70c81ad",
   "metadata": {},
   "source": [
    "## 2. Data Quality Assessment\n",
    "\n",
    "Now let's assess the quality of our data by checking for missing values, duplicates, data type issues, and encoding problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4804786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MISSING VALUES ANALYSIS\n",
    "# ============================================================================\n",
    "# Create a comprehensive missing values report\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percent = (missing_data / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_data.index,\n",
    "    'Missing Count': missing_data.values,\n",
    "    'Missing Percentage': missing_percent.values\n",
    "}).sort_values('Missing Count', ascending=False)\n",
    "\n",
    "# Filter to show only columns with missing values\n",
    "missing_df = missing_df[missing_df['Missing Count'] > 0]\n",
    "\n",
    "print(\"Missing Values Analysis:\")\n",
    "print(\"=\"*80)\n",
    "if len(missing_df) > 0:\n",
    "    print(missing_df.to_string(index=False))\n",
    "    print(f\"\\n‚ö†Ô∏è  Total columns with missing values: {len(missing_df)}\")\n",
    "else:\n",
    "    print(\"‚úÖ No missing values found in the dataset!\")\n",
    "\n",
    "# Visualize missing values if any exist\n",
    "if len(missing_df) > 0:\n",
    "    fig = px.bar(\n",
    "        missing_df,\n",
    "        x='Column',\n",
    "        y='Missing Percentage',\n",
    "        title='Missing Values Percentage by Column',\n",
    "        labels={'Missing Percentage': 'Missing %', 'Column': 'Column Name'},\n",
    "        color='Missing Percentage',\n",
    "        color_continuous_scale='Reds'\n",
    "    )\n",
    "    fig.update_layout(height=600, xaxis_tickangle=-45)\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690f315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DUPLICATE ROWS CHECK\n",
    "# ============================================================================\n",
    "# Check for completely duplicate rows\n",
    "duplicate_rows = df.duplicated().sum()\n",
    "print(\"Duplicate Rows Analysis:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total duplicate rows: {duplicate_rows:,} ({duplicate_rows/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Check for duplicate Order IDs (should be unique per order)\n",
    "if 'Order Id' in df.columns:\n",
    "    duplicate_order_ids = df['Order Id'].duplicated().sum()\n",
    "    print(f\"\\nDuplicate Order IDs: {duplicate_order_ids:,}\")\n",
    "    if duplicate_order_ids > 0:\n",
    "        print(\"\\n‚ö†Ô∏è  Warning: Some Order IDs appear multiple times - investigating...\")\n",
    "        # Show examples of duplicate Order IDs\n",
    "        dup_ids = df[df['Order Id'].duplicated(keep=False)]['Order Id'].unique()[:5]\n",
    "        print(f\"\\nSample duplicate Order IDs: {dup_ids}\")\n",
    "        print(\"\\nSample rows with duplicate Order IDs:\")\n",
    "        print(df[df['Order Id'].isin(dup_ids[:3])][['Order Id', 'Order Item Id', 'Product Card Id']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e09247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA TYPE VERIFICATION\n",
    "# ============================================================================\n",
    "# Check for columns that should be numeric but are stored as objects\n",
    "print(\"Data Type Verification:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Identify potential numeric columns stored as objects\n",
    "potential_numeric = []\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    # Try to convert to numeric (excluding NaN)\n",
    "    try:\n",
    "        # Sample some non-null values\n",
    "        sample = df[col].dropna().head(100)\n",
    "        if len(sample) > 0:\n",
    "            # Try numeric conversion\n",
    "            pd.to_numeric(sample, errors='raise')\n",
    "            potential_numeric.append(col)\n",
    "    except (ValueError, TypeError):\n",
    "        pass\n",
    "\n",
    "if potential_numeric:\n",
    "    print(f\"‚ö†Ô∏è  Potential numeric columns stored as object: {potential_numeric}\")\n",
    "else:\n",
    "    print(\"‚úÖ No obvious numeric columns stored as object type\")\n",
    "\n",
    "# Check date columns\n",
    "date_columns = [col for col in df.columns if 'date' in col.lower() or 'Date' in col]\n",
    "print(f\"\\nüìÖ Date-related columns found: {date_columns}\")\n",
    "print(\"\\nSample values from date columns:\")\n",
    "for col in date_columns:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Data type: {df[col].dtype}\")\n",
    "    print(f\"  Sample values: {df[col].dropna().head(3).tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22db519b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ENCODING ISSUES CHECK\n",
    "# ============================================================================\n",
    "# Check for encoding issues in text columns (common with special characters)\n",
    "print(\"Encoding Issues Check:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check country names for encoding issues\n",
    "if 'Customer Country' in df.columns:\n",
    "    print(\"Customer Country unique values:\")\n",
    "    countries = df['Customer Country'].value_counts()\n",
    "    print(countries.head(20))\n",
    "\n",
    "    # Look for suspicious values that might indicate encoding issues\n",
    "    suspicious = [c for c in countries.index if any(char in str(c) for char in ['', '√É', '√¢', '√©'])]\n",
    "    if suspicious:\n",
    "        print(f\"\\n‚ö†Ô∏è  Potentially problematic country names (encoding issues): {suspicious[:10]}\")\n",
    "\n",
    "if 'Order Country' in df.columns:\n",
    "    print(\"\\nOrder Country unique values:\")\n",
    "    order_countries = df['Order Country'].value_counts()\n",
    "    print(order_countries.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64f5ad5",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis\n",
    "\n",
    "Now let's dive deeper into the data with visualizations and statistical analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb78fde",
   "metadata": {},
   "source": [
    "### 3.1 Numeric Columns Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90f6962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CORRELATION MATRIX\n",
    "# ============================================================================\n",
    "# Calculate correlation matrix for numeric columns\n",
    "print(\"Correlation Analysis:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select only numeric columns for correlation\n",
    "numeric_df = df[numeric_cols].copy()\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = numeric_df.corr()\n",
    "\n",
    "# Visualize correlation matrix as heatmap\n",
    "fig = px.imshow(\n",
    "    corr_matrix,\n",
    "    color_continuous_scale='RdBu',\n",
    "    aspect=\"auto\",\n",
    "    title=\"Correlation Matrix of Numeric Features\",\n",
    "    labels=dict(color=\"Correlation\")\n",
    ")\n",
    "fig.update_layout(height=800, width=800)\n",
    "fig.show()\n",
    "\n",
    "# Find highly correlated pairs (absolute correlation > 0.7)\n",
    "print(\"\\nHighly Correlated Feature Pairs (|correlation| > 0.7):\")\n",
    "high_corr_pairs = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        corr_val = corr_matrix.iloc[i, j]\n",
    "        if abs(corr_val) > 0.7:\n",
    "            high_corr_pairs.append((\n",
    "                corr_matrix.columns[i],\n",
    "                corr_matrix.columns[j],\n",
    "                corr_val\n",
    "            ))\n",
    "\n",
    "if high_corr_pairs:\n",
    "    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Feature 1', 'Feature 2', 'Correlation'])\n",
    "    print(high_corr_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"No highly correlated pairs found (threshold: 0.7)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56099398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DISTRIBUTION ANALYSIS FOR KEY NUMERIC FEATURES\n",
    "# ============================================================================\n",
    "# Analyze distributions of important numeric features\n",
    "key_numeric_features = [\n",
    "    'Sales', 'Order Item Total', 'Order Item Quantity',\n",
    "    'Days for shipping (real)', 'Days for shipment (scheduled)',\n",
    "    'Benefit per order', 'Sales per customer'\n",
    "]\n",
    "\n",
    "# Filter to features that exist in the dataset\n",
    "key_features = [f for f in key_numeric_features if f in df.columns]\n",
    "\n",
    "if key_features:\n",
    "    # Create subplots for distributions\n",
    "    n_cols = 3\n",
    "    n_rows = (len(key_features) + n_cols - 1) // n_cols\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=n_rows,\n",
    "        cols=n_cols,\n",
    "        subplot_titles=key_features,\n",
    "        vertical_spacing=0.08\n",
    "    )\n",
    "\n",
    "    for idx, feature in enumerate(key_features):\n",
    "        row = (idx // n_cols) + 1\n",
    "        col = (idx % n_cols) + 1\n",
    "\n",
    "        # Create histogram\n",
    "        fig.add_trace(\n",
    "            go.Histogram(\n",
    "                x=df[feature].dropna(),\n",
    "                nbinsx=50,\n",
    "                name=feature,\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=row, col=col\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=300 * n_rows,\n",
    "        title_text=\"Distribution of Key Numeric Features\"\n",
    "    )\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1c1e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OUTLIER DETECTION USING IQR METHOD\n",
    "# ============================================================================\n",
    "# Detect outliers in numeric columns using Interquartile Range (IQR) method\n",
    "print(\"Outlier Detection (IQR Method):\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "outlier_summary = []\n",
    "\n",
    "for col in numeric_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]\n",
    "    outlier_count = len(outliers)\n",
    "    outlier_percent = (outlier_count / len(df)) * 100\n",
    "\n",
    "    if outlier_count > 0:\n",
    "        outlier_summary.append({\n",
    "            'Column': col,\n",
    "            'Outlier Count': outlier_count,\n",
    "            'Outlier Percentage': f\"{outlier_percent:.2f}%\",\n",
    "            'Lower Bound': lower_bound,\n",
    "            'Upper Bound': upper_bound,\n",
    "            'Min Value': df[col].min(),\n",
    "            'Max Value': df[col].max()\n",
    "        })\n",
    "\n",
    "if outlier_summary:\n",
    "    outlier_df = pd.DataFrame(outlier_summary)\n",
    "    print(outlier_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"No outliers detected using IQR method.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c008402",
   "metadata": {},
   "source": [
    "### 3.2 Categorical Columns Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88ba7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DELIVERY STATUS ANALYSIS\n",
    "# ============================================================================\n",
    "# Analyze delivery status distribution\n",
    "if 'Delivery Status' in df.columns:\n",
    "    print(\"Delivery Status Distribution:\")\n",
    "    print(\"=\"*80)\n",
    "    delivery_status = df['Delivery Status'].value_counts()\n",
    "    print(delivery_status)\n",
    "    print(f\"\\nPercentage distribution:\")\n",
    "    print((delivery_status / len(df) * 100).round(2))\n",
    "\n",
    "    # Visualize delivery status\n",
    "    fig = px.pie(\n",
    "        values=delivery_status.values,\n",
    "        names=delivery_status.index,\n",
    "        title=\"Distribution of Delivery Status\"\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "    # Analyze Late Delivery Risk\n",
    "    if 'Late_delivery_risk' in df.columns:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Late Delivery Risk Distribution:\")\n",
    "        late_delivery = df['Late_delivery_risk'].value_counts()\n",
    "        print(late_delivery)\n",
    "        print(f\"\\nPercentage: {(late_delivery / len(df) * 100).round(2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70b37b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SHIPPING MODE ANALYSIS\n",
    "# ============================================================================\n",
    "# Analyze shipping mode distribution and its relationship with delivery status\n",
    "if 'Shipping Mode' in df.columns:\n",
    "    print(\"Shipping Mode Distribution:\")\n",
    "    print(\"=\"*80)\n",
    "    shipping_mode = df['Shipping Mode'].value_counts()\n",
    "    print(shipping_mode)\n",
    "\n",
    "    # Visualize shipping modes\n",
    "    fig = px.bar(\n",
    "        x=shipping_mode.index,\n",
    "        y=shipping_mode.values,\n",
    "        title=\"Distribution of Shipping Modes\",\n",
    "        labels={'x': 'Shipping Mode', 'y': 'Count'}\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "    # Cross-tabulation: Shipping Mode vs Delivery Status\n",
    "    if 'Delivery Status' in df.columns:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Cross-tabulation: Shipping Mode vs Delivery Status\")\n",
    "        crosstab = pd.crosstab(df['Shipping Mode'], df['Delivery Status'], margins=True)\n",
    "        print(crosstab)\n",
    "\n",
    "        # Visualize the relationship\n",
    "        crosstab_pct = pd.crosstab(df['Shipping Mode'], df['Delivery Status'], normalize='index') * 100\n",
    "        fig = px.bar(\n",
    "            crosstab_pct,\n",
    "            barmode='group',\n",
    "            title=\"Delivery Status by Shipping Mode (Percentages)\",\n",
    "            labels={'value': 'Percentage', 'Shipping Mode': 'Shipping Mode'}\n",
    "        )\n",
    "        fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe88c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CUSTOMER SEGMENT ANALYSIS\n",
    "# ============================================================================\n",
    "# Analyze customer segments\n",
    "if 'Customer Segment' in df.columns:\n",
    "    print(\"Customer Segment Distribution:\")\n",
    "    print(\"=\"*80)\n",
    "    customer_segment = df['Customer Segment'].value_counts()\n",
    "    print(customer_segment)\n",
    "\n",
    "    fig = px.pie(\n",
    "        values=customer_segment.values,\n",
    "        names=customer_segment.index,\n",
    "        title=\"Distribution of Customer Segments\"\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "    # Analyze sales by customer segment\n",
    "    if 'Sales' in df.columns:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Average Sales by Customer Segment:\")\n",
    "        sales_by_segment = df.groupby('Customer Segment')['Sales'].agg(['mean', 'median', 'sum'])\n",
    "        print(sales_by_segment.round(2))\n",
    "\n",
    "        fig = px.box(\n",
    "            df,\n",
    "            x='Customer Segment',\n",
    "            y='Sales',\n",
    "            title=\"Sales Distribution by Customer Segment\"\n",
    "        )\n",
    "        fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b39d2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PRODUCT CATEGORY ANALYSIS\n",
    "# ============================================================================\n",
    "# Analyze product categories\n",
    "if 'Category Name' in df.columns:\n",
    "    print(\"Top 20 Product Categories by Order Count:\")\n",
    "    print(\"=\"*80)\n",
    "    category_counts = df['Category Name'].value_counts().head(20)\n",
    "    print(category_counts)\n",
    "\n",
    "    fig = px.bar(\n",
    "        x=category_counts.values,\n",
    "        y=category_counts.index,\n",
    "        orientation='h',\n",
    "        title=\"Top 20 Product Categories\",\n",
    "        labels={'x': 'Order Count', 'y': 'Category Name'}\n",
    "    )\n",
    "    fig.update_layout(height=600)\n",
    "    fig.show()\n",
    "\n",
    "    # Analyze sales by category\n",
    "    if 'Sales' in df.columns:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Top 20 Categories by Total Sales:\")\n",
    "        sales_by_category = df.groupby('Category Name')['Sales'].sum().sort_values(ascending=False).head(20)\n",
    "        print(sales_by_category.round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cd3ded",
   "metadata": {},
   "source": [
    "### 3.3 Temporal Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61ec3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PARSE DATE COLUMNS\n",
    "# ============================================================================\n",
    "# Create a copy of the dataframe for date parsing\n",
    "df_temp = df.copy()\n",
    "\n",
    "# Identify date columns\n",
    "date_columns = [col for col in df.columns if 'date' in col.lower()]\n",
    "\n",
    "print(\"Date Column Parsing:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Parse date columns\n",
    "date_parsed = {}\n",
    "for col in date_columns:\n",
    "    try:\n",
    "        # Try parsing with the format seen in the data\n",
    "        df_temp[col] = pd.to_datetime(df_temp[col], format='%m/%d/%Y %H:%M', errors='coerce')\n",
    "        date_parsed[col] = True\n",
    "        print(f\"‚úÖ {col}: Successfully parsed\")\n",
    "        print(f\"   Date range: {df_temp[col].min()} to {df_temp[col].max()}\")\n",
    "        print(f\"   Missing dates: {df_temp[col].isnull().sum()} ({df_temp[col].isnull().sum()/len(df_temp)*100:.2f}%)\")\n",
    "    except Exception as e:\n",
    "        date_parsed[col] = False\n",
    "        print(f\"‚ùå {col}: Failed to parse - {str(e)}\")\n",
    "\n",
    "# Store parsed dates for temporal analysis\n",
    "df_dates = df_temp[date_columns].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5f7eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TIME SERIES TRENDS\n",
    "# ============================================================================\n",
    "# Analyze sales trends over time if order date is available\n",
    "order_date_col = None\n",
    "for col in date_columns:\n",
    "    if 'order' in col.lower() and 'date' in col.lower():\n",
    "        order_date_col = col\n",
    "        break\n",
    "\n",
    "if order_date_col and df_temp[order_date_col].notna().sum() > 0:\n",
    "    # Create time series data\n",
    "    df_ts = df_temp.copy()\n",
    "    df_ts = df_ts[df_ts[order_date_col].notna()]\n",
    "    df_ts['Order_Date_Parsed'] = df_temp[order_date_col]\n",
    "    df_ts['Year'] = df_ts['Order_Date_Parsed'].dt.year\n",
    "    df_ts['Month'] = df_ts['Order_Date_Parsed'].dt.month\n",
    "    df_ts['YearMonth'] = df_ts['Order_Date_Parsed'].dt.to_period('M')\n",
    "\n",
    "    # Aggregate sales by month\n",
    "    if 'Sales' in df_ts.columns:\n",
    "        monthly_sales = df_ts.groupby('YearMonth')['Sales'].agg(['sum', 'mean', 'count'])\n",
    "        monthly_sales.index = monthly_sales.index.astype(str)\n",
    "\n",
    "        print(\"Monthly Sales Trends:\")\n",
    "        print(\"=\"*80)\n",
    "        print(monthly_sales.head(10))\n",
    "\n",
    "        # Visualize time series\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=monthly_sales.index,\n",
    "            y=monthly_sales['sum'],\n",
    "            mode='lines+markers',\n",
    "            name='Total Sales',\n",
    "            line=dict(width=2)\n",
    "        ))\n",
    "        fig.update_layout(\n",
    "            title=\"Monthly Sales Trend\",\n",
    "            xaxis_title=\"Month\",\n",
    "            yaxis_title=\"Total Sales\",\n",
    "            height=500\n",
    "        )\n",
    "        fig.show()\n",
    "\n",
    "        # Analyze seasonality by month\n",
    "        df_ts['Month_Name'] = df_ts['Order_Date_Parsed'].dt.strftime('%B')\n",
    "        monthly_avg_sales = df_ts.groupby('Month_Name')['Sales'].mean().sort_values(ascending=False)\n",
    "\n",
    "        fig = px.bar(\n",
    "            x=monthly_avg_sales.index,\n",
    "            y=monthly_avg_sales.values,\n",
    "            title=\"Average Sales by Month (Seasonality Analysis)\",\n",
    "            labels={'x': 'Month', 'y': 'Average Sales'}\n",
    "        )\n",
    "        fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccf3ac5",
   "metadata": {},
   "source": [
    "### 3.4 Geographic Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57756a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GEOGRAPHIC DISTRIBUTION ANALYSIS\n",
    "# ============================================================================\n",
    "# Analyze customer and order geographic distribution\n",
    "\n",
    "# Customer countries\n",
    "if 'Customer Country' in df.columns:\n",
    "    print(\"Top 20 Customer Countries:\")\n",
    "    print(\"=\"*80)\n",
    "    customer_countries = df['Customer Country'].value_counts().head(20)\n",
    "    print(customer_countries)\n",
    "\n",
    "    fig = px.bar(\n",
    "        x=customer_countries.values,\n",
    "        y=customer_countries.index,\n",
    "        orientation='h',\n",
    "        title=\"Top 20 Customer Countries\",\n",
    "        labels={'x': 'Customer Count', 'y': 'Country'}\n",
    "    )\n",
    "    fig.update_layout(height=600)\n",
    "    fig.show()\n",
    "\n",
    "# Order regions\n",
    "if 'Order Region' in df.columns:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Order Regions Distribution:\")\n",
    "    order_regions = df['Order Region'].value_counts()\n",
    "    print(order_regions)\n",
    "\n",
    "    fig = px.pie(\n",
    "        values=order_regions.values,\n",
    "        names=order_regions.index,\n",
    "        title=\"Distribution of Orders by Region\"\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "    # Sales by region\n",
    "    if 'Sales' in df.columns:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Total Sales by Order Region:\")\n",
    "        sales_by_region = df.groupby('Order Region')['Sales'].sum().sort_values(ascending=False)\n",
    "        print(sales_by_region.round(2))\n",
    "\n",
    "        fig = px.bar(\n",
    "            x=sales_by_region.index,\n",
    "            y=sales_by_region.values,\n",
    "            title=\"Total Sales by Order Region\",\n",
    "            labels={'x': 'Region', 'y': 'Total Sales'}\n",
    "        )\n",
    "        fig.update_layout(xaxis_tickangle=-45)\n",
    "        fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7b9cb5",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning Operations\n",
    "\n",
    "Now we'll perform comprehensive data cleaning based on the issues identified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de768bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CREATE CLEANED DATASET COPY\n",
    "# ============================================================================\n",
    "# Create a copy of the original dataframe for cleaning\n",
    "# We'll preserve the original df for comparison\n",
    "df_clean = df.copy()\n",
    "\n",
    "print(\"Starting data cleaning process...\")\n",
    "print(f\"Initial shape: {df_clean.shape}\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a748af3",
   "metadata": {},
   "source": [
    "### 4.1 Handle Missing Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa346a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MISSING VALUES HANDLING STRATEGY\n",
    "# ============================================================================\n",
    "# Document and handle missing values based on their nature and importance\n",
    "\n",
    "missing_before = df_clean.isnull().sum().sum()\n",
    "print(\"Handling Missing Values:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Strategy 1: Drop columns with too many missing values (>50%)\n",
    "cols_to_drop = []\n",
    "for col in df_clean.columns:\n",
    "    missing_pct = (df_clean[col].isnull().sum() / len(df_clean)) * 100\n",
    "    if missing_pct > 50:\n",
    "        cols_to_drop.append(col)\n",
    "        print(f\"  Dropping column '{col}' ({missing_pct:.2f}% missing)\")\n",
    "\n",
    "if cols_to_drop:\n",
    "    df_clean = df_clean.drop(columns=cols_to_drop)\n",
    "    print(f\"‚úÖ Dropped {len(cols_to_drop)} columns with >50% missing values\")\n",
    "\n",
    "# Strategy 2: Handle missing values in specific columns\n",
    "# Check for columns that might benefit from imputation\n",
    "\n",
    "# For numeric columns: use median (more robust to outliers)\n",
    "numeric_missing = df_clean.select_dtypes(include=[np.number]).columns[\n",
    "    df_clean.select_dtypes(include=[np.number]).isnull().any()\n",
    "].tolist()\n",
    "\n",
    "for col in numeric_missing:\n",
    "    missing_count = df_clean[col].isnull().sum()\n",
    "    if missing_count > 0 and missing_count < len(df_clean) * 0.5:  # If <50% missing\n",
    "        median_val = df_clean[col].median()\n",
    "        df_clean[col].fillna(median_val, inplace=True)\n",
    "        print(f\"  Filled {missing_count} missing values in '{col}' with median: {median_val:.2f}\")\n",
    "\n",
    "# For categorical columns: use mode (most frequent value)\n",
    "categorical_missing = df_clean.select_dtypes(include=['object']).columns[\n",
    "    df_clean.select_dtypes(include=['object']).isnull().any()\n",
    "].tolist()\n",
    "\n",
    "for col in categorical_missing:\n",
    "    missing_count = df_clean[col].isnull().sum()\n",
    "    if missing_count > 0 and missing_count < len(df_clean) * 0.5:  # If <50% missing\n",
    "        mode_val = df_clean[col].mode()[0] if len(df_clean[col].mode()) > 0 else 'Unknown'\n",
    "        df_clean[col].fillna(mode_val, inplace=True)\n",
    "        print(f\"  Filled {missing_count} missing values in '{col}' with mode: '{mode_val}'\")\n",
    "\n",
    "missing_after = df_clean.isnull().sum().sum()\n",
    "print(f\"\\n‚úÖ Missing values handled: {missing_before:,} ‚Üí {missing_after:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d538177f",
   "metadata": {},
   "source": [
    "### 4.2 Handle Duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd580d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DUPLICATE HANDLING\n",
    "# ============================================================================\n",
    "duplicates_before = df_clean.duplicated().sum()\n",
    "print(\"Handling Duplicates:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Remove completely duplicate rows\n",
    "if duplicates_before > 0:\n",
    "    df_clean = df_clean.drop_duplicates()\n",
    "    print(f\"‚úÖ Removed {duplicates_before:,} completely duplicate rows\")\n",
    "else:\n",
    "    print(\"‚úÖ No completely duplicate rows found\")\n",
    "\n",
    "# Note: If Order ID appears multiple times, it might be legitimate (one order can have multiple items)\n",
    "# We'll keep those as they represent different order items\n",
    "print(f\"\\nShape after duplicate removal: {df_clean.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ed841e",
   "metadata": {},
   "source": [
    "### 4.3 Data Type Conversions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3778e603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONVERT DATE COLUMNS TO DATETIME\n",
    "# ============================================================================\n",
    "print(\"Converting Date Columns:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "date_columns = [col for col in df_clean.columns if 'date' in col.lower()]\n",
    "\n",
    "for col in date_columns:\n",
    "    try:\n",
    "        # Convert to datetime with the format from the data\n",
    "        df_clean[col] = pd.to_datetime(df_clean[col], format='%m/%d/%Y %H:%M', errors='coerce')\n",
    "        print(f\"‚úÖ Converted '{col}' to datetime\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not convert '{col}': {str(e)}\")\n",
    "\n",
    "# Check for any remaining date columns that might need conversion\n",
    "print(f\"\\nDate columns after conversion: {[col for col in df_clean.columns if 'date' in col.lower()]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e266bb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ENSURE NUMERIC COLUMNS ARE PROPERLY TYPED\n",
    "# ============================================================================\n",
    "print(\"Verifying Numeric Columns:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Convert any numeric columns that might be stored as object\n",
    "for col in df_clean.select_dtypes(include=['object']).columns:\n",
    "    # Try to convert to numeric\n",
    "    try:\n",
    "        # Sample conversion test\n",
    "        sample = df_clean[col].dropna().head(100)\n",
    "        if len(sample) > 0:\n",
    "            pd.to_numeric(sample, errors='raise')\n",
    "            # If successful, convert the whole column\n",
    "            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "            print(f\"‚úÖ Converted '{col}' to numeric\")\n",
    "    except (ValueError, TypeError):\n",
    "        pass\n",
    "\n",
    "print(\"‚úÖ Numeric column verification complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729f0fd6",
   "metadata": {},
   "source": [
    "### 4.4 Standardize Categorical Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e928af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STANDARDIZE COUNTRY NAMES\n",
    "# ============================================================================\n",
    "print(\"Standardizing Country Names:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create a mapping for common country name variations\n",
    "country_mapping = {\n",
    "    'EE. UU.': 'USA',\n",
    "    'EE.UU.': 'USA',\n",
    "    'Estados Unidos': 'USA',\n",
    "    'United States': 'USA',\n",
    "    # Add more mappings as needed based on the data\n",
    "}\n",
    "\n",
    "# Apply mapping to Customer Country\n",
    "if 'Customer Country' in df_clean.columns:\n",
    "    before = df_clean['Customer Country'].value_counts()\n",
    "    df_clean['Customer Country'] = df_clean['Customer Country'].replace(country_mapping)\n",
    "    after = df_clean['Customer Country'].value_counts()\n",
    "    print(\"‚úÖ Standardized Customer Country names\")\n",
    "\n",
    "# Apply mapping to Order Country\n",
    "if 'Order Country' in df_clean.columns:\n",
    "    df_clean['Order Country'] = df_clean['Order Country'].replace(country_mapping)\n",
    "    print(\"‚úÖ Standardized Order Country names\")\n",
    "\n",
    "print(\"\\nNote: Some encoding issues with special characters may persist if data was corrupted during download.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4034ffdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# NORMALIZE TEXT FIELDS\n",
    "# ============================================================================\n",
    "print(\"Normalizing Text Fields:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Trim whitespace from string columns\n",
    "text_columns = df_clean.select_dtypes(include=['object']).columns\n",
    "for col in text_columns:\n",
    "    # Skip columns that might be intentionally formatted (like emails, passwords)\n",
    "    if 'email' not in col.lower() and 'password' not in col.lower():\n",
    "        df_clean[col] = df_clean[col].astype(str).str.strip()\n",
    "        print(f\"  Trimmed whitespace in '{col}'\")\n",
    "\n",
    "print(\"‚úÖ Text normalization complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787abbb5",
   "metadata": {},
   "source": [
    "### 4.5 Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dedabf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CREATE DERIVED DATE FEATURES\n",
    "# ============================================================================\n",
    "print(\"Creating Derived Date Features:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find order date column\n",
    "order_date_col = None\n",
    "for col in df_clean.columns:\n",
    "    if 'order' in col.lower() and 'date' in col.lower() and df_clean[col].dtype == 'datetime64[ns]':\n",
    "        order_date_col = col\n",
    "        break\n",
    "\n",
    "if order_date_col:\n",
    "    # Extract temporal features\n",
    "    df_clean['Order_Year'] = df_clean[order_date_col].dt.year\n",
    "    df_clean['Order_Month'] = df_clean[order_date_col].dt.month\n",
    "    df_clean['Order_Day'] = df_clean[order_date_col].dt.day\n",
    "    df_clean['Order_DayOfWeek'] = df_clean[order_date_col].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "    df_clean['Order_DayOfYear'] = df_clean[order_date_col].dt.dayofyear\n",
    "    df_clean['Order_Quarter'] = df_clean[order_date_col].dt.quarter\n",
    "    df_clean['Order_Week'] = df_clean[order_date_col].dt.isocalendar().week\n",
    "\n",
    "    print(f\"‚úÖ Created temporal features from '{order_date_col}'\")\n",
    "    print(f\"   Features: Order_Year, Order_Month, Order_Day, Order_DayOfWeek, Order_DayOfYear, Order_Quarter, Order_Week\")\n",
    "\n",
    "# Find shipping date column\n",
    "shipping_date_col = None\n",
    "for col in df_clean.columns:\n",
    "    if 'shipping' in col.lower() and 'date' in col.lower() and df_clean[col].dtype == 'datetime64[ns]':\n",
    "        shipping_date_col = col\n",
    "        break\n",
    "\n",
    "if shipping_date_col:\n",
    "    df_clean['Shipping_Year'] = df_clean[shipping_date_col].dt.year\n",
    "    df_clean['Shipping_Month'] = df_clean[shipping_date_col].dt.month\n",
    "    df_clean['Shipping_DayOfWeek'] = df_clean[shipping_date_col].dt.dayofweek\n",
    "    print(f\"‚úÖ Created temporal features from '{shipping_date_col}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f27ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CALCULATE SHIPPING DELAY\n",
    "# ============================================================================\n",
    "print(\"Calculating Shipping Delay:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate the difference between actual and scheduled shipping days\n",
    "if 'Days for shipping (real)' in df_clean.columns and 'Days for shipment (scheduled)' in df_clean.columns:\n",
    "    df_clean['Shipping_Delay_Days'] = (\n",
    "        df_clean['Days for shipping (real)'] -\n",
    "        df_clean['Days for shipment (scheduled)']\n",
    "    )\n",
    "    print(\"‚úÖ Created 'Shipping_Delay_Days' feature\")\n",
    "    print(f\"   Positive values = late delivery, Negative = early delivery\")\n",
    "    print(f\"   Statistics:\")\n",
    "    print(f\"   - Mean: {df_clean['Shipping_Delay_Days'].mean():.2f} days\")\n",
    "    print(f\"   - Median: {df_clean['Shipping_Delay_Days'].median():.2f} days\")\n",
    "    print(f\"   - Late deliveries (>0): {(df_clean['Shipping_Delay_Days'] > 0).sum():,} ({(df_clean['Shipping_Delay_Days'] > 0).mean()*100:.2f}%)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Could not calculate shipping delay - required columns missing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744ebf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CREATE ADDITIONAL DERIVED FEATURES\n",
    "# ============================================================================\n",
    "print(\"Creating Additional Derived Features:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate profit margin if possible\n",
    "if 'Order Item Total' in df_clean.columns and 'Order Item Product Price' in df_clean.columns:\n",
    "    # Check if we can calculate margin\n",
    "    if 'Order Item Discount' in df_clean.columns:\n",
    "        df_clean['Item_Net_Price'] = df_clean['Order Item Product Price'] - df_clean['Order Item Discount']\n",
    "        print(\"‚úÖ Created 'Item_Net_Price' (Price - Discount)\")\n",
    "\n",
    "# Calculate order total value per customer if multiple items per order\n",
    "if 'Order Id' in df_clean.columns and 'Order Item Total' in df_clean.columns:\n",
    "    df_clean['Order_Total_Value'] = df_clean.groupby('Order Id')['Order Item Total'].transform('sum')\n",
    "    print(\"‚úÖ Created 'Order_Total_Value' (sum of all items in order)\")\n",
    "\n",
    "# Create binary flags for important categories\n",
    "if 'Delivery Status' in df_clean.columns:\n",
    "    df_clean['Is_Late_Delivery'] = (df_clean['Delivery Status'] == 'Late delivery').astype(int)\n",
    "    df_clean['Is_Advance_Shipping'] = (df_clean['Delivery Status'] == 'Advance shipping').astype(int)\n",
    "    print(\"‚úÖ Created binary flags for delivery status\")\n",
    "\n",
    "print(\"\\n‚úÖ Feature engineering complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d041490f",
   "metadata": {},
   "source": [
    "## 5. Data Validation and Summary\n",
    "\n",
    "Let's validate the cleaned data and compare it with the original dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8011820e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA VALIDATION - BEFORE AND AFTER COMPARISON\n",
    "# ============================================================================\n",
    "print(\"Data Cleaning Summary:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Original shape: {df.shape}\")\n",
    "print(f\"Cleaned shape: {df_clean.shape}\")\n",
    "print(f\"Rows removed: {df.shape[0] - df_clean.shape[0]:,}\")\n",
    "print(f\"Columns removed: {df.shape[1] - df_clean.shape[1]}\")\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "print(f\"  Before: {df.isnull().sum().sum():,}\")\n",
    "print(f\"  After: {df_clean.isnull().sum().sum():,}\")\n",
    "\n",
    "print(\"\\nDuplicate Rows:\")\n",
    "print(f\"  Before: {df.duplicated().sum():,}\")\n",
    "print(f\"  After: {df_clean.duplicated().sum():,}\")\n",
    "\n",
    "print(\"\\n‚úÖ Data cleaning validation complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a669b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FINAL DATA QUALITY CHECK\n",
    "# ============================================================================\n",
    "print(\"Final Data Quality Check:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check for any remaining issues\n",
    "issues = []\n",
    "\n",
    "# Check for infinite values\n",
    "inf_cols = []\n",
    "for col in df_clean.select_dtypes(include=[np.number]).columns:\n",
    "    if np.isinf(df_clean[col]).any():\n",
    "        inf_cols.append(col)\n",
    "if inf_cols:\n",
    "    issues.append(f\"Infinite values found in: {inf_cols}\")\n",
    "\n",
    "# Check for columns with all null values\n",
    "all_null_cols = df_clean.columns[df_clean.isnull().all()].tolist()\n",
    "if all_null_cols:\n",
    "    issues.append(f\"Columns with all null values: {all_null_cols}\")\n",
    "\n",
    "# Check for columns with zero variance (constant columns)\n",
    "zero_var_cols = []\n",
    "for col in df_clean.select_dtypes(include=[np.number]).columns:\n",
    "    if df_clean[col].nunique() == 1:\n",
    "        zero_var_cols.append(col)\n",
    "if zero_var_cols:\n",
    "    issues.append(f\"Constant columns (zero variance): {zero_var_cols}\")\n",
    "\n",
    "if issues:\n",
    "    print(\"‚ö†Ô∏è  Issues found:\")\n",
    "    for issue in issues:\n",
    "        print(f\"  - {issue}\")\n",
    "else:\n",
    "    print(\"‚úÖ No major data quality issues detected!\")\n",
    "\n",
    "print(f\"\\nFinal dataset shape: {df_clean.shape}\")\n",
    "print(f\"Memory usage: {df_clean.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce802fcf",
   "metadata": {},
   "source": [
    "## 6. Feature Selection for Machine Learning Models\n",
    "\n",
    "Now we'll analyze features for two types of models:\n",
    "1. **Forecasting Models**: Predict future sales, demand, or quantities\n",
    "2. **Classification Models**: Predict late delivery risk, order status, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dead20f",
   "metadata": {},
   "source": [
    "### 6.1 Target Variable Identification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8186987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IDENTIFY POTENTIAL TARGET VARIABLES\n",
    "# ============================================================================\n",
    "print(\"Target Variable Identification:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä CLASSIFICATION TARGETS (Categorical Prediction):\")\n",
    "classification_targets = []\n",
    "if 'Late_delivery_risk' in df_clean.columns:\n",
    "    classification_targets.append('Late_delivery_risk')\n",
    "    print(f\"  ‚úÖ Late_delivery_risk: {df_clean['Late_delivery_risk'].value_counts().to_dict()}\")\n",
    "if 'Delivery Status' in df_clean.columns:\n",
    "    classification_targets.append('Delivery Status')\n",
    "    print(f\"  ‚úÖ Delivery Status: {df_clean['Delivery Status'].nunique()} unique values\")\n",
    "if 'Order Status' in df_clean.columns:\n",
    "    classification_targets.append('Order Status')\n",
    "    print(f\"  ‚úÖ Order Status: {df_clean['Order Status'].nunique()} unique values\")\n",
    "\n",
    "print(\"\\nüìà FORECASTING TARGETS (Numeric Prediction/Time Series):\")\n",
    "forecasting_targets = []\n",
    "if 'Sales' in df_clean.columns:\n",
    "    forecasting_targets.append('Sales')\n",
    "    print(f\"  ‚úÖ Sales: Range [{df_clean['Sales'].min():.2f}, {df_clean['Sales'].max():.2f}]\")\n",
    "if 'Order Item Total' in df_clean.columns:\n",
    "    forecasting_targets.append('Order Item Total')\n",
    "    print(f\"  ‚úÖ Order Item Total: Range [{df_clean['Order Item Total'].min():.2f}, {df_clean['Order Item Total'].max():.2f}]\")\n",
    "if 'Order Item Quantity' in df_clean.columns:\n",
    "    forecasting_targets.append('Order Item Quantity')\n",
    "    print(f\"  ‚úÖ Order Item Quantity: Range [{df_clean['Order Item Quantity'].min()}, {df_clean['Order Item Quantity'].max()}]\")\n",
    "\n",
    "print(f\"\\nPrimary Classification Target: {classification_targets[0] if classification_targets else 'None'}\")\n",
    "print(f\"Primary Forecasting Target: {forecasting_targets[0] if forecasting_targets else 'None'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d8c367",
   "metadata": {},
   "source": [
    "### 6.2 Feature Importance Analysis for Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8384a6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FEATURE IMPORTANCE FOR CLASSIFICATION (Late Delivery Risk)\n",
    "# ============================================================================\n",
    "print(\"Feature Importance Analysis for Classification (Late Delivery Risk):\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare data for feature importance analysis\n",
    "if 'Late_delivery_risk' in df_clean.columns:\n",
    "    # Select features for classification\n",
    "    # Exclude target variables and potential data leakage features\n",
    "    exclude_cols = [\n",
    "        'Late_delivery_risk', 'Delivery Status', 'Order Status',\n",
    "        'Customer Email', 'Customer Password',  # Sensitive info\n",
    "        'shipping date (DateOrders)',  # This could be leakage for predicting late delivery\n",
    "        'Days for shipping (real)',  # This is the actual outcome\n",
    "    ]\n",
    "\n",
    "    # Get numeric features\n",
    "    numeric_features = [col for col in df_clean.select_dtypes(include=[np.number]).columns\n",
    "                       if col not in exclude_cols]\n",
    "\n",
    "    # Get categorical features\n",
    "    categorical_features = [col for col in df_clean.select_dtypes(include=['object']).columns\n",
    "                          if col not in exclude_cols and col not in ['Customer Email', 'Customer Password']]\n",
    "\n",
    "    # Create feature dataframe\n",
    "    X_class = df_clean[numeric_features].copy()\n",
    "\n",
    "    # Encode categorical features using label encoding for importance analysis\n",
    "    le_dict = {}\n",
    "    for col in categorical_features[:10]:  # Limit to top 10 categorical features for speed\n",
    "        if df_clean[col].nunique() < 100:  # Only encode if not too many categories\n",
    "            le = LabelEncoder()\n",
    "            X_class[f'{col}_encoded'] = le.fit_transform(df_clean[col].astype(str))\n",
    "            le_dict[col] = le\n",
    "\n",
    "    # Target variable\n",
    "    y_class = df_clean['Late_delivery_risk'].copy()\n",
    "\n",
    "    # Remove any remaining NaN\n",
    "    mask = ~(X_class.isnull().any(axis=1) | y_class.isnull())\n",
    "    X_class_clean = X_class[mask]\n",
    "    y_class_clean = y_class[mask]\n",
    "\n",
    "    print(f\"Features for analysis: {len(X_class_clean.columns)}\")\n",
    "    print(f\"Valid samples: {len(X_class_clean):,}\")\n",
    "\n",
    "    # Calculate mutual information (measures dependency between features and target)\n",
    "    if len(X_class_clean) > 0 and len(X_class_clean.columns) > 0:\n",
    "        mi_scores = mutual_info_classif(X_class_clean, y_class_clean, random_state=42, n_neighbors=3)\n",
    "\n",
    "        # Create feature importance dataframe\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': X_class_clean.columns,\n",
    "            'Mutual_Information': mi_scores\n",
    "        }).sort_values('Mutual_Information', ascending=False)\n",
    "\n",
    "        print(\"\\nTop 20 Most Important Features for Late Delivery Risk Prediction:\")\n",
    "        print(feature_importance.head(20).to_string(index=False))\n",
    "\n",
    "        # Visualize feature importance\n",
    "        fig = px.bar(\n",
    "            feature_importance.head(20),\n",
    "            x='Mutual_Information',\n",
    "            y='Feature',\n",
    "            orientation='h',\n",
    "            title=\"Top 20 Features for Late Delivery Risk Prediction (Mutual Information)\",\n",
    "            labels={'Mutual_Information': 'Mutual Information Score', 'Feature': 'Feature Name'}\n",
    "        )\n",
    "        fig.update_layout(height=600)\n",
    "        fig.show()\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Insufficient data for feature importance analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb8ea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CORRELATION ANALYSIS WITH TARGET VARIABLE (Classification)\n",
    "# ============================================================================\n",
    "# Analyze correlation between numeric features and late delivery risk\n",
    "if 'Late_delivery_risk' in df_clean.columns:\n",
    "    print(\"\\nCorrelation Analysis with Late Delivery Risk:\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Calculate correlation with target\n",
    "    numeric_cols_class = [col for col in numeric_features if col in df_clean.columns]\n",
    "    correlations = df_clean[numeric_cols_class + ['Late_delivery_risk']].corr()['Late_delivery_risk'].sort_values(key=abs, ascending=False)\n",
    "\n",
    "    # Remove the target itself\n",
    "    correlations = correlations[correlations.index != 'Late_delivery_risk']\n",
    "\n",
    "    print(\"Top 20 Features Correlated with Late Delivery Risk:\")\n",
    "    print(correlations.head(20).to_string())\n",
    "\n",
    "    # Visualize\n",
    "    fig = px.bar(\n",
    "        x=correlations.head(20).values,\n",
    "        y=correlations.head(20).index,\n",
    "        orientation='h',\n",
    "        title=\"Top 20 Features Correlated with Late Delivery Risk\",\n",
    "        labels={'x': 'Correlation Coefficient', 'y': 'Feature'}\n",
    "    )\n",
    "    fig.update_layout(height=600)\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69829dba",
   "metadata": {},
   "source": [
    "### 6.3 Feature Selection for Classification Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656fb99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RECOMMENDED FEATURES FOR CLASSIFICATION (Late Delivery Risk)\n",
    "# ============================================================================\n",
    "print(\"Recommended Feature Set for Classification (Late Delivery Risk):\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define feature categories\n",
    "classification_features = {\n",
    "    'Temporal Features': [\n",
    "        'Days for shipment (scheduled)',\n",
    "        'Order_Year', 'Order_Month', 'Order_DayOfWeek', 'Order_Quarter'\n",
    "    ],\n",
    "    'Shipping Features': [\n",
    "        'Shipping Mode',\n",
    "        'Shipping_Delay_Days',  # If we can calculate it from scheduled\n",
    "        'Order Region'\n",
    "    ],\n",
    "    'Order Characteristics': [\n",
    "        'Order Item Quantity',\n",
    "        'Order Item Total',\n",
    "        'Order_Total_Value',\n",
    "        'Order Item Discount Rate'\n",
    "    ],\n",
    "    'Product Features': [\n",
    "        'Product Category Id',\n",
    "        'Category Name',\n",
    "        'Department Name',\n",
    "        'Product Price'\n",
    "    ],\n",
    "    'Customer Features': [\n",
    "        'Customer Segment',\n",
    "        'Customer Country',\n",
    "        'Customer State'\n",
    "    ],\n",
    "    'Geographic Features': [\n",
    "        'Latitude', 'Longitude',\n",
    "        'Market',\n",
    "        'Order Country'\n",
    "    ],\n",
    "    'Payment Features': [\n",
    "        'Type'  # Payment type\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Filter to features that exist in the dataset\n",
    "recommended_class_features = []\n",
    "for category, features in classification_features.items():\n",
    "    existing_features = [f for f in features if f in df_clean.columns]\n",
    "    if existing_features:\n",
    "        print(f\"\\n{category}:\")\n",
    "        for feat in existing_features:\n",
    "            print(f\"  ‚úÖ {feat}\")\n",
    "            recommended_class_features.append(feat)\n",
    "\n",
    "print(f\"\\nüìã Total Recommended Features: {len(recommended_class_features)}\")\n",
    "\n",
    "# Identify features to exclude (potential data leakage)\n",
    "leakage_features = [\n",
    "    'Days for shipping (real)',  # This is the actual outcome\n",
    "    'shipping date (DateOrders)',  # Future information\n",
    "    'Delivery Status',  # Directly related to late delivery\n",
    "    'Is_Late_Delivery',  # Derived from Delivery Status\n",
    "]\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  Features to EXCLUDE (Data Leakage Risk):\")\n",
    "for feat in leakage_features:\n",
    "    if feat in df_clean.columns:\n",
    "        print(f\"  ‚ùå {feat}\")\n",
    "\n",
    "# Final feature list\n",
    "final_class_features = [f for f in recommended_class_features if f not in leakage_features]\n",
    "print(f\"\\n‚úÖ Final Feature Set ({len(final_class_features)} features) for Classification:\")\n",
    "for i, feat in enumerate(final_class_features[:30], 1):  # Show first 30\n",
    "    print(f\"  {i:2d}. {feat}\")\n",
    "if len(final_class_features) > 30:\n",
    "    print(f\"  ... and {len(final_class_features) - 30} more features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1670c461",
   "metadata": {},
   "source": [
    "### 6.4 Feature Selection for Forecasting Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfceaf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FEATURE IMPORTANCE FOR FORECASTING (Sales Prediction)\n",
    "# ============================================================================\n",
    "print(\"Feature Importance Analysis for Forecasting (Sales Prediction):\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'Sales' in df_clean.columns:\n",
    "    # Prepare features for forecasting\n",
    "    exclude_cols_forecast = [\n",
    "        'Sales', 'Order Item Total',  # These are directly related to the target\n",
    "        'Sales per customer',  # Derived from sales\n",
    "        'Benefit per order',  # Outcome, not predictor\n",
    "        'Customer Email', 'Customer Password',\n",
    "        'shipping date (DateOrders)',  # Future information\n",
    "        'Delivery Status',  # Outcome\n",
    "    ]\n",
    "\n",
    "    # Get numeric features for forecasting\n",
    "    numeric_features_forecast = [col for col in df_clean.select_dtypes(include=[np.number]).columns\n",
    "                                if col not in exclude_cols_forecast]\n",
    "\n",
    "    # Get categorical features\n",
    "    categorical_features_forecast = [col for col in df_clean.select_dtypes(include=['object']).columns\n",
    "                                    if col not in exclude_cols_forecast and\n",
    "                                    col not in ['Customer Email', 'Customer Password']]\n",
    "\n",
    "    # Create feature dataframe\n",
    "    X_forecast = df_clean[numeric_features_forecast].copy()\n",
    "\n",
    "    # Encode categorical features\n",
    "    for col in categorical_features_forecast[:10]:  # Limit for speed\n",
    "        if df_clean[col].nunique() < 100:\n",
    "            le = LabelEncoder()\n",
    "            X_forecast[f'{col}_encoded'] = le.fit_transform(df_clean[col].astype(str))\n",
    "\n",
    "    # Target variable\n",
    "    y_forecast = df_clean['Sales'].copy()\n",
    "\n",
    "    # Remove NaN\n",
    "    mask = ~(X_forecast.isnull().any(axis=1) | y_forecast.isnull())\n",
    "    X_forecast_clean = X_forecast[mask]\n",
    "    y_forecast_clean = y_forecast[mask]\n",
    "\n",
    "    print(f\"Features for analysis: {len(X_forecast_clean.columns)}\")\n",
    "    print(f\"Valid samples: {len(X_forecast_clean):,}\")\n",
    "\n",
    "    # Calculate mutual information for regression\n",
    "    if len(X_forecast_clean) > 0 and len(X_forecast_clean.columns) > 0:\n",
    "        mi_scores_forecast = mutual_info_regression(X_forecast_clean, y_forecast_clean, random_state=42, n_neighbors=3)\n",
    "\n",
    "        # Create feature importance dataframe\n",
    "        feature_importance_forecast = pd.DataFrame({\n",
    "            'Feature': X_forecast_clean.columns,\n",
    "            'Mutual_Information': mi_scores_forecast\n",
    "        }).sort_values('Mutual_Information', ascending=False)\n",
    "\n",
    "        print(\"\\nTop 20 Most Important Features for Sales Forecasting:\")\n",
    "        print(feature_importance_forecast.head(20).to_string(index=False))\n",
    "\n",
    "        # Visualize\n",
    "        fig = px.bar(\n",
    "            feature_importance_forecast.head(20),\n",
    "            x='Mutual_Information',\n",
    "            y='Feature',\n",
    "            orientation='h',\n",
    "            title=\"Top 20 Features for Sales Forecasting (Mutual Information)\",\n",
    "            labels={'Mutual_Information': 'Mutual Information Score', 'Feature': 'Feature Name'}\n",
    "        )\n",
    "        fig.update_layout(height=600)\n",
    "        fig.show()\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Insufficient data for feature importance analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d853ca58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RECOMMENDED FEATURES FOR FORECASTING (Sales/Demand)\n",
    "# ============================================================================\n",
    "print(\"Recommended Feature Set for Forecasting (Sales/Demand):\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "forecasting_features = {\n",
    "    'Temporal Features (Critical for Time Series)': [\n",
    "        'Order_Year', 'Order_Month', 'Order_Day', 'Order_DayOfWeek',\n",
    "        'Order_DayOfYear', 'Order_Quarter', 'Order_Week'\n",
    "    ],\n",
    "    'Product Features': [\n",
    "        'Product Category Id',\n",
    "        'Category Name',\n",
    "        'Department Name',\n",
    "        'Product Card Id',\n",
    "        'Product Price'\n",
    "    ],\n",
    "    'Customer Features': [\n",
    "        'Customer Segment',\n",
    "        'Customer Country',\n",
    "        'Customer State'\n",
    "    ],\n",
    "    'Geographic Features': [\n",
    "        'Order Region',\n",
    "        'Order Country',\n",
    "        'Market',\n",
    "        'Latitude', 'Longitude'\n",
    "    ],\n",
    "    'Historical Features (to be created)': [\n",
    "        'Sales_Lag_7',  # Sales 7 days ago\n",
    "        'Sales_Lag_30',  # Sales 30 days ago\n",
    "        'Sales_MovingAvg_7',  # 7-day moving average\n",
    "        'Sales_MovingAvg_30',  # 30-day moving average\n",
    "        'Category_Sales_Trend',  # Trend for this category\n",
    "    ],\n",
    "    'External Factors': [\n",
    "        'Order Item Discount Rate',\n",
    "        'Shipping Mode',\n",
    "        'Type'  # Payment type\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Filter to existing features\n",
    "recommended_forecast_features = []\n",
    "for category, features in forecasting_features.items():\n",
    "    existing_features = [f for f in features if f in df_clean.columns]\n",
    "    if existing_features:\n",
    "        print(f\"\\n{category}:\")\n",
    "        for feat in existing_features:\n",
    "            print(f\"  ‚úÖ {feat}\")\n",
    "            recommended_forecast_features.append(feat)\n",
    "    elif 'Historical' in category:\n",
    "        print(f\"\\n{category}:\")\n",
    "        print(f\"  üìù These features need to be created during feature engineering\")\n",
    "\n",
    "print(f\"\\nüìã Total Recommended Features (available now): {len(recommended_forecast_features)}\")\n",
    "print(f\"\\nüìù Additional Features to Create:\")\n",
    "print(\"   - Lag features (previous sales values)\")\n",
    "print(\"   - Moving averages\")\n",
    "print(\"   - Seasonal indicators\")\n",
    "print(\"   - Category/region-specific trends\")\n",
    "\n",
    "print(f\"\\n‚úÖ Final Feature Set ({len(recommended_forecast_features)} features) for Forecasting:\")\n",
    "for i, feat in enumerate(recommended_forecast_features[:30], 1):\n",
    "    print(f\"  {i:2d}. {feat}\")\n",
    "if len(recommended_forecast_features) > 30:\n",
    "    print(f\"  ... and {len(recommended_forecast_features) - 30} more features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4859dc7dc201970d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
